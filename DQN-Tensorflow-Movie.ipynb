{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of DQN in Tensorflow\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/matplotlib/__init__.py:1401: UserWarning:  This call to matplotlib.use() has no effect\n",
      "because the backend has already been chosen;\n",
      "matplotlib.use() must be called *before* pylab, matplotlib.pyplot,\n",
      "or matplotlib.backends is imported for the first time.\n",
      "\n",
      "  warnings.warn(_use_error_msg)\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from util import *\n",
    "from dlg_manager import *\n",
    "from alg import *\n",
    "from agent import *\n",
    "from user_sim import *\n",
    "from state_tracker import *\n",
    "import random\n",
    "from config import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nlg import *\n",
    "from six.moves import cPickle as pickle\n",
    "import IPython\n",
    "import copy, argparse, json\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 11\n",
      "Sample of dict:\n",
      "- not_sure: 10\n",
      "- greeting: 4\n",
      "- thanks: 7\n",
      "- confirm_question: 2\n",
      "- confirm_answer: 3\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "act_set_path = './data/dia_acts.txt'\n",
    "act_set = text_to_dict(act_set_path)\n",
    "sample_dict(act_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slot set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 29\n",
      "Sample of dict:\n",
      "- ticket: 27\n",
      "- price: 18\n",
      "- critic_rating: 4\n",
      "- actor: 0\n",
      "- result: 26\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "slots_set_path = \"./data/slot_set.txt\"\n",
    "slot_set = text_to_dict(slots_set_path)\n",
    "sample_dict(slot_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### movie dic: info about movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 991\n",
      "Sample of dict:\n",
      "- 651: {'city': 'du quoin', 'state': 'illinois', 'theater_chain': 'amc showplace carbondale 8', 'starttime': 'anytime after 6pm', 'genre': 'thriller science fiction', 'moviename': 'star wars', 'date': 'Friday the 11th', 'distanceconstraints': 'vicinity'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "movie_kb_path = \"./data/movie_kb.1k.p\"\n",
    "movie_kb = pickle.load(open(movie_kb_path, 'rb'), encoding=\"latin\")\n",
    "sample_dict(movie_kb, sample_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generator (pretrained)\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "nlg_model_path = './data/trained_model/nlg/lstm_tanh_relu_[1468202263.38]_2_0.610.p'\n",
    "nlg_model = Nlg()\n",
    "nlg_model.load_nlg_model(nlg_model_path)\n",
    "diaact_nl_pairs_path = \"./data/nlg/dia_act_nl_pairs.v6.json\"\n",
    "nlg_model.load_predefine_act_nl_pairs(diaact_nl_pairs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_model_dir: .\\checkpoints\\template\\07102016\\\n",
      "slot_set: data/slot_set.txt\n",
      "cv_fold: 6\n",
      "sdgtype: rmsprop\n",
      "learning_rate: 0.001\n",
      "reg_cost: 0.001\n",
      "model: lstm_tanh\n",
      "activation_func: relu\n",
      "decay_rate: 0.999\n",
      "batch_size: 16\n",
      "smooth_eps: 1e-08\n",
      "split_method: 1\n",
      "eva_metric: 2\n",
      "data_path: .\\data\\movieMultiLine.Annot.Corrected.Final.v3.csv\n",
      "momentum: 0.1\n",
      "save_check_point: 20\n",
      "dia_slot_val: 2\n",
      "trained_model_path: None\n",
      "act_set: data/dia_acts.txt\n",
      "valid_test: 0\n",
      "max_epochs: 200\n",
      "check_point: 20\n",
      "hidden_size: 100\n",
      "grad_clip: -0.0001\n",
      "init_rnn: 0\n",
      "feed_recurrence: 0\n",
      "slot_rep: 1\n",
      "pretrained_model_path: None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "model_params = pickle.load(open(nlg_model_path, 'rb'), encoding='latin1')\n",
    "params = model_params['params']\n",
    "params['batch_size'] = 16\n",
    "batch_size = 16\n",
    "save_check_point = 20\n",
    "params['trained_model_path'] = None\n",
    "for k in params:\n",
    "    print(\"{}: {}\".format(k, params[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Simulator\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goals length: 128\n",
      "Sample the first goal: \n",
      "{'inform_slots': {'moviename': 'zootopia', 'city': 'birmingham', 'theater': 'carmike summit 16', 'state': 'al', 'starttime': 'around 2pm', 'numberofpeople': '1', 'date': 'today'}, 'request_slots': {}, 'diaact': 'request'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "goal_file_path = './data/user_goals_first_turn_template.part.movie.v1.p'\n",
    "all_goal_set = pickle.load(open(goal_file_path, 'rb'), encoding=\"latin\")\n",
    "print(\"goals length: {}\".format(len(all_goal_set)))\n",
    "print(\"Sample the first goal: \\n{}\".format(all_goal_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split goal set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "0\n",
      "26\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# split goal set\n",
    "split_fold = params.get('split_fold', 5)\n",
    "goal_set = {'train':[], 'valid':[], 'test':[], 'all':[]}\n",
    "for u_goal_id, u_goal in enumerate(all_goal_set):\n",
    "    if u_goal_id % split_fold == 1: goal_set['test'].append(u_goal)\n",
    "    else: goal_set['train'].append(u_goal)\n",
    "    goal_set['all'].append(u_goal)\n",
    "print(len(goal_set['train']))\n",
    "print(len(goal_set['valid']))\n",
    "print(len(goal_set['test']))\n",
    "print(len(goal_set['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user simulator param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "usersim_params = {}\n",
    "usersim_params['max_turn'] = 40\n",
    "usersim_params['slot_err_prob'] = 0.00\n",
    "# slot_err_mode: 0 for slot_val only; 1 for three errs\n",
    "usersim_params['slot_err_mode'] = 0\n",
    "usersim_params['intent_err_prob'] = 0\n",
    "# run_mode: 0 for default NL; 1 for dia_act; 2 for both\n",
    "usersim_params['run_mode'] = 0\n",
    "# 0 for dia_act level; 1 for NL level\n",
    "usersim_params['act_level'] = 0\n",
    "# train/test/all; default is all\n",
    "usersim_params['learn_phase'] = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a movie dictionary for user simulator - slot:possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 20\n",
      "Sample of dict:\n",
      "- description: ['is still playing in theaters', 'violence', 'violent', 'good intelligent', 'The critics consensus is that it is Fast funny and gleefully profane the fourth-wall-busting Deadpool subverts superhero film formula with wildly entertaining -- and decidedly non-family-friendly -- results', 'disney', 'A woman (Mary Elizabeth Winstead) discovers the horrifying truth about the outside world while living in an underground shelter with two men (John Goodman John Gallagher Jr)', 'highest rated pizza', \"Mortal hero Bek teams with the god Horus in an alliance against Set the merciless god of darkness who has usurped Egypt's throne plunging the once peaceful and prosperous empire into chaos and conflict\", 'scary']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "movie_dict_path = './data/user/dicts.v3.p'\n",
    "movie_dictionary = pickle.load(open(movie_dict_path, 'rb'), encoding=\"latin\")\n",
    "samples = sample_dict(movie_dictionary, sample_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = RuleSimulator(movie_dictionary, act_set, slot_set, goal_set, usersim_params)\n",
    "user.set_nlg_model(nlg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained model path = None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "agent_params = {}\n",
    "# maximum length of each dialog (default=20, 0=no maximum length)\n",
    "agent_params['max_turn'] = 40\n",
    "# Epsilon to determine stochasticity of epsilon-greedy agent policies\n",
    "agent_params['epsilon'] = 0\n",
    "# run_mode: 0 for default NL; 1 for dia_act; 2 for both\n",
    "agent_params['agent_run_mode'] = 3\n",
    "# 0 for dia_act level; 1 for NL level\n",
    "agent_params['agent_act_level'] = 0\n",
    "\n",
    "############### DQN #################\n",
    "# the size for experience replay\n",
    "agent_params['experience_replay_pool_size'] = 10000\n",
    "# # the hidden size for DQN\n",
    "agent_params['dqn_hidden_size'] = 60\n",
    "agent_params['batch_size'] = 16\n",
    "# # gamma for DQN\n",
    "agent_params['gamma'] = 0.9\n",
    "# # predict model for DQN\n",
    "agent_params['predict_mode'] = True\n",
    "agent_params['trained_model_path'] = params['pretrained_model_path']\n",
    "#####################################\n",
    "print(\"pretrained model path = {}\".format(agent_params['trained_model_path']))\n",
    "# 0: no warm start; 1: warm start for training\n",
    "agent_params['warm_start'] = 1\n",
    "# run_mode: 0 for NL; 1 for dia_act\n",
    "agent_params['cmd_input_mode'] = 0\n",
    "\n",
    "success_rate_threshold = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_cardinality  11\n",
      "feasible_actions 43\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# agent = RequestBasicsAgent(movie_kb, act_set, slot_set, agent_params)\n",
    "# agent = AgentDQN(movie_kb, act_set, slot_set, agent_params)\n",
    "# agt = 9\n",
    "agt = 10\n",
    "agent_params['batch_size']  = batch_size\n",
    "if agt == 9:\n",
    "    agent = AgentDQN(movie_kb, act_set, slot_set, agent_params)\n",
    "else:\n",
    "    agent = DQNAgentTF(movie_kb, act_set, slot_set, agent_params)\n",
    "\n",
    "agent.set_nlg_model(nlg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog Manager\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dlg_manager = DlgManager(agent, user, act_set, slot_set, movie_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Episodes\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "status = {'successes': 0, 'count': 0, 'cumulative_reward': 0}\n",
    "# the size of validation set\n",
    "simulation_epoch_size = 100\n",
    "# the number of epochs for warm start \n",
    "warm_start_epochs = 200\n",
    "# num_episodes = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Warm_Start Simulation (by Rule Policy) \"\"\"\n",
    "def warm_start_simulation():\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    res = {}\n",
    "    for episode in range(warm_start_epochs):\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "            if episode_over:\n",
    "                if reward > 0: \n",
    "                    successes += 1\n",
    "#                     print (\"warm_start simulation episode %s: Success\" % (episode))\n",
    "#                 else: print (\"warm_start simulation episode %s: Fail\" % (episode))\n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "        \n",
    "        if len(agent.experience_replay_pool) >= agent.experience_replay_pool_size:\n",
    "            break\n",
    "    \n",
    "    agent.warm_start = 2\n",
    "    res['success_rate'] = float(successes)/simulation_epoch_size\n",
    "    res['ave_reward'] = float(cumulative_reward)/simulation_epoch_size\n",
    "    res['ave_turns'] = float(cumulative_turns)/simulation_epoch_size\n",
    "    print (\"Warm_Start %s epochs, success rate %s, ave reward %s, ave turns %s\" % (episode+1, res['success_rate'], res['ave_reward'], res['ave_turns']))\n",
    "    print (\"Current experience replay buffer size %s\" % (len(agent.experience_replay_pool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulation_epoch(simulation_epoch_size):\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    res = {}\n",
    "    for episode in range(simulation_epoch_size):\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "            if episode_over:\n",
    "                if reward > 0: \n",
    "                    successes += 1\n",
    "#                     print (\"simulation episode %s: Success\" % (episode))\n",
    "#                 else: print (\"simulation episode %s: Fail\" % (episode))\n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "    \n",
    "    res['success_rate'] = float(successes)/simulation_epoch_size\n",
    "    res['ave_reward'] = float(cumulative_reward)/simulation_epoch_size\n",
    "    res['ave_turns'] = float(cumulative_turns)/simulation_epoch_size\n",
    "    print(\"simulation success rate %s, ave reward %s, ave turns %s\" % (res['success_rate'], res['ave_reward'], res['ave_turns']))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_episodes(count, status):\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    \n",
    "    if agt >= 9 and params['trained_model_path'] == None and agent.warm_start == 1:\n",
    "        print ('warm_start starting ...')\n",
    "        warm_start_simulation()\n",
    "        print ('warm_start finished, start RL training ...')\n",
    "    \n",
    "    for episode in range(count):\n",
    "        print (\"----------------- Episode: %s ----------------- \" % (episode))\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        \n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "                \n",
    "            if episode_over:\n",
    "                if reward > 0:\n",
    "                    print (\"Successful Dialog!\")\n",
    "                    successes += 1\n",
    "#                 else: print (\"Failed Dialog!\")\n",
    "                \n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "        \n",
    "        # simulation\n",
    "        if agt >= 9 and params['trained_model_path'] == None:\n",
    "            agent.predict_mode = True\n",
    "            simulation_res = simulation_epoch(simulation_epoch_size)\n",
    "            \n",
    "            performance_records['success_rate'][episode] = simulation_res['success_rate']\n",
    "            performance_records['ave_turns'][episode] = simulation_res['ave_turns']\n",
    "            performance_records['ave_reward'][episode] = simulation_res['ave_reward']\n",
    "            \n",
    "            if simulation_res['success_rate'] >= best_res['success_rate']:\n",
    "                if simulation_res['success_rate'] >= success_rate_threshold: # threshold = 0.30\n",
    "                    agent.experience_replay_pool = [] \n",
    "                    simulation_epoch(simulation_epoch_size)\n",
    "                \n",
    "#             if simulation_res['success_rate'] > best_res['success_rate']:\n",
    "#                 best_model['model'] = copy.deepcopy(agent)\n",
    "#                 best_res['success_rate'] = simulation_res['success_rate']\n",
    "#                 best_res['ave_reward'] = simulation_res['ave_reward']\n",
    "#                 best_res['ave_turns'] = simulation_res['ave_turns']\n",
    "#                 best_res['epoch'] = episode\n",
    "                \n",
    "            loss = agent.train(batch_size, 1)\n",
    "            if agt == 10: \n",
    "                agent.model.update_target_params()\n",
    "            else: \n",
    "                agent.clone_dqn = copy.deepcopy(agent.dqn)\n",
    "                \n",
    "            agent.predict_mode = False\n",
    "            \n",
    "            print (\"Simulation success rate %s, Ave reward %s, Ave turns %s, Best success rate %s\" % (performance_records['success_rate'][episode], performance_records['ave_reward'][episode], performance_records['ave_turns'][episode], best_res['success_rate']))\n",
    "#             if episode % save_check_point == 0 and params['trained_model_path'] == None: # save the model every 10 episodes\n",
    "#                 save_model(params['write_model_dir'], agt, best_res['success_rate'], best_model['model'], best_res['epoch'], episode)\n",
    "#                 save_performance_records(params['write_model_dir'], agt, performance_records)\n",
    "        curve.append(successes/(episode+1))\n",
    "        losses.append(loss)\n",
    "        print(\"Progress: %s / %s, Success rate: %s / %s Avg reward: %.2f Avg turns: %.2f\" % (episode+1, count, successes, episode+1, float(cumulative_reward)/(episode+1), float(cumulative_turns)/(episode+1)))\n",
    "    print(\"Success rate: %s / %s Avg reward: %.2f Avg turns: %.2f\" % (successes, count, float(cumulative_reward)/count, float(cumulative_turns)/count))\n",
    "    status['successes'] += successes\n",
    "    status['count'] += count\n",
    "    \n",
    "#     if agt == 9 and params['traained_model_path'] == None:\n",
    "#         save_model(params['write_model_dir'], agt, float(successes)/count, best_model['model'], best_res['epoch'], count)\n",
    "#         save_performance_records(params['write_model_dir'], agt, performance_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Warm Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Eval\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm_start starting ...\n",
      "Warm_Start 200 epochs, success rate 0.78, ave reward -0.4, ave turns 32.0\n",
      "Current experience replay buffer size 1600\n",
      "warm_start finished, start RL training ...\n",
      "----------------- Episode: 0 ----------------- \n",
      "simulation success rate 0.0, ave reward -56.4, ave turns 34.8\n",
      "Train on : 3361\n",
      "- cur bellman err 15.9684, experience replay pool 3361\n",
      "Simulation success rate 0.0, Ave reward -56.4, Ave turns 34.8, Best success rate 0\n",
      "Progress: 1 / 100, Success rate: 0 / 1 Avg reward: -60.00 Avg turns: 42.00\n",
      "----------------- Episode: 1 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 5461\n",
      "- cur bellman err 10.7747, experience replay pool 5461\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 2 / 100, Success rate: 0 / 2 Avg reward: -60.00 Avg turns: 42.00\n",
      "----------------- Episode: 2 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 7561\n",
      "- cur bellman err 8.2090, experience replay pool 7561\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 3 / 100, Success rate: 0 / 3 Avg reward: -60.00 Avg turns: 42.00\n",
      "----------------- Episode: 3 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 9661\n",
      "- cur bellman err 6.8512, experience replay pool 9661\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 4 / 100, Success rate: 0 / 4 Avg reward: -60.00 Avg turns: 42.00\n",
      "----------------- Episode: 4 ----------------- \n",
      "simulation success rate 0.0, ave reward -55.72, ave turns 33.44\n",
      "Train on : 11333\n",
      "- cur bellman err 6.1815, experience replay pool 11333\n",
      "Simulation success rate 0.0, Ave reward -55.72, Ave turns 33.44, Best success rate 0\n",
      "Progress: 5 / 100, Success rate: 0 / 5 Avg reward: -57.00 Avg turns: 36.00\n",
      "----------------- Episode: 5 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 13433\n",
      "- cur bellman err 5.2574, experience replay pool 13433\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 6 / 100, Success rate: 0 / 6 Avg reward: -57.50 Avg turns: 37.00\n",
      "----------------- Episode: 6 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 15533\n",
      "- cur bellman err 5.3708, experience replay pool 15533\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 7 / 100, Success rate: 0 / 7 Avg reward: -57.86 Avg turns: 37.71\n",
      "----------------- Episode: 7 ----------------- \n",
      "simulation success rate 0.0, ave reward -58.25, ave turns 38.5\n",
      "Train on : 17458\n",
      "- cur bellman err 4.8750, experience replay pool 17458\n",
      "Simulation success rate 0.0, Ave reward -58.25, Ave turns 38.5, Best success rate 0\n",
      "Progress: 8 / 100, Success rate: 0 / 8 Avg reward: -58.12 Avg turns: 38.25\n",
      "----------------- Episode: 8 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 19558\n",
      "- cur bellman err 5.4587, experience replay pool 19558\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 9 / 100, Success rate: 0 / 9 Avg reward: -58.33 Avg turns: 38.67\n",
      "----------------- Episode: 9 ----------------- \n",
      "simulation success rate 0.0, ave reward -57.96, ave turns 37.92\n",
      "Train on : 21454\n",
      "- cur bellman err 5.1397, experience replay pool 21454\n",
      "Simulation success rate 0.0, Ave reward -57.96, Ave turns 37.92, Best success rate 0\n",
      "Progress: 10 / 100, Success rate: 0 / 10 Avg reward: -58.50 Avg turns: 39.00\n",
      "----------------- Episode: 10 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 23554\n",
      "- cur bellman err 5.4939, experience replay pool 23554\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 11 / 100, Success rate: 0 / 11 Avg reward: -58.64 Avg turns: 39.27\n",
      "----------------- Episode: 11 ----------------- \n",
      "simulation success rate 0.02, ave reward -56.84, ave turns 40.48\n",
      "Train on : 25578\n",
      "- cur bellman err 5.6577, experience replay pool 25578\n",
      "Simulation success rate 0.02, Ave reward -56.84, Ave turns 40.48, Best success rate 0\n",
      "Progress: 12 / 100, Success rate: 0 / 12 Avg reward: -58.75 Avg turns: 39.50\n",
      "----------------- Episode: 12 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 27678\n",
      "- cur bellman err 5.6735, experience replay pool 27678\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 13 / 100, Success rate: 0 / 13 Avg reward: -58.85 Avg turns: 39.69\n",
      "----------------- Episode: 13 ----------------- \n",
      "simulation success rate 0.01, ave reward -56.54, ave turns 37.48\n",
      "Train on : 29552\n",
      "- cur bellman err 5.9672, experience replay pool 29552\n",
      "Simulation success rate 0.01, Ave reward -56.54, Ave turns 37.48, Best success rate 0\n",
      "Progress: 14 / 100, Success rate: 0 / 14 Avg reward: -58.93 Avg turns: 39.86\n",
      "----------------- Episode: 14 ----------------- \n",
      "simulation success rate 0.0, ave reward -56.91, ave turns 35.82\n",
      "Train on : 31343\n",
      "- cur bellman err 6.0281, experience replay pool 31343\n",
      "Simulation success rate 0.0, Ave reward -56.91, Ave turns 35.82, Best success rate 0\n",
      "Progress: 15 / 100, Success rate: 0 / 15 Avg reward: -59.00 Avg turns: 40.00\n",
      "----------------- Episode: 15 ----------------- \n",
      "simulation success rate 0.0, ave reward -59.85, ave turns 41.7\n",
      "Train on : 33428\n",
      "- cur bellman err 5.6921, experience replay pool 33428\n",
      "Simulation success rate 0.0, Ave reward -59.85, Ave turns 41.7, Best success rate 0\n",
      "Progress: 16 / 100, Success rate: 0 / 16 Avg reward: -59.06 Avg turns: 40.12\n",
      "----------------- Episode: 16 ----------------- \n",
      "simulation success rate 0.05, ave reward -49.17, ave turns 32.34\n",
      "Train on : 35045\n",
      "- cur bellman err 5.8894, experience replay pool 35045\n",
      "Simulation success rate 0.05, Ave reward -49.17, Ave turns 32.34, Best success rate 0\n",
      "Progress: 17 / 100, Success rate: 0 / 17 Avg reward: -59.12 Avg turns: 40.24\n",
      "----------------- Episode: 17 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 37145\n",
      "- cur bellman err 5.4368, experience replay pool 37145\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 18 / 100, Success rate: 0 / 18 Avg reward: -59.17 Avg turns: 40.33\n",
      "----------------- Episode: 18 ----------------- \n",
      "simulation success rate 0.0, ave reward -58.17, ave turns 38.34\n",
      "Train on : 39062\n",
      "- cur bellman err 5.3024, experience replay pool 39062\n",
      "Simulation success rate 0.0, Ave reward -58.17, Ave turns 38.34, Best success rate 0\n",
      "Progress: 19 / 100, Success rate: 0 / 19 Avg reward: -59.21 Avg turns: 40.42\n",
      "----------------- Episode: 19 ----------------- \n",
      "simulation success rate 0.0, ave reward -59.85, ave turns 41.7\n",
      "Train on : 41147\n",
      "- cur bellman err 5.1969, experience replay pool 41147\n",
      "Simulation success rate 0.0, Ave reward -59.85, Ave turns 41.7, Best success rate 0\n",
      "Progress: 20 / 100, Success rate: 0 / 20 Avg reward: -59.25 Avg turns: 40.50\n",
      "----------------- Episode: 20 ----------------- \n",
      "simulation success rate 0.0, ave reward -58.36, ave turns 38.72\n",
      "Train on : 43083\n",
      "- cur bellman err 5.2408, experience replay pool 43083\n",
      "Simulation success rate 0.0, Ave reward -58.36, Ave turns 38.72, Best success rate 0\n",
      "Progress: 21 / 100, Success rate: 0 / 21 Avg reward: -59.29 Avg turns: 40.57\n",
      "----------------- Episode: 21 ----------------- \n",
      "simulation success rate 0.0, ave reward -58.63, ave turns 39.26\n",
      "Train on : 45046\n",
      "- cur bellman err 5.2098, experience replay pool 45046\n",
      "Simulation success rate 0.0, Ave reward -58.63, Ave turns 39.26, Best success rate 0\n",
      "Progress: 22 / 100, Success rate: 0 / 22 Avg reward: -59.32 Avg turns: 40.64\n",
      "----------------- Episode: 22 ----------------- \n",
      "simulation success rate 0.01, ave reward -54.53, ave turns 33.46\n",
      "Train on : 46719\n",
      "- cur bellman err 5.2066, experience replay pool 46719\n",
      "Simulation success rate 0.01, Ave reward -54.53, Ave turns 33.46, Best success rate 0\n",
      "Progress: 23 / 100, Success rate: 0 / 23 Avg reward: -59.35 Avg turns: 40.70\n",
      "----------------- Episode: 23 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 48819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- cur bellman err 4.7538, experience replay pool 48819\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 24 / 100, Success rate: 0 / 24 Avg reward: -59.38 Avg turns: 40.75\n",
      "----------------- Episode: 24 ----------------- \n",
      "simulation success rate 0.0, ave reward -59.77, ave turns 41.54\n",
      "Train on : 50896\n",
      "- cur bellman err 4.7550, experience replay pool 50896\n",
      "Simulation success rate 0.0, Ave reward -59.77, Ave turns 41.54, Best success rate 0\n",
      "Progress: 25 / 100, Success rate: 0 / 25 Avg reward: -59.40 Avg turns: 40.80\n",
      "----------------- Episode: 25 ----------------- \n",
      "simulation success rate 0.0, ave reward -59.5, ave turns 41.0\n",
      "Train on : 52946\n",
      "- cur bellman err 4.8165, experience replay pool 52946\n",
      "Simulation success rate 0.0, Ave reward -59.5, Ave turns 41.0, Best success rate 0\n",
      "Progress: 26 / 100, Success rate: 0 / 26 Avg reward: -59.42 Avg turns: 40.85\n",
      "----------------- Episode: 26 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 55046\n",
      "- cur bellman err 4.5313, experience replay pool 55046\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 27 / 100, Success rate: 0 / 27 Avg reward: -59.44 Avg turns: 40.89\n",
      "----------------- Episode: 27 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 57146\n",
      "- cur bellman err 4.4861, experience replay pool 57146\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 28 / 100, Success rate: 0 / 28 Avg reward: -59.46 Avg turns: 40.93\n",
      "----------------- Episode: 28 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 59246\n",
      "- cur bellman err 4.4556, experience replay pool 59246\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 29 / 100, Success rate: 0 / 29 Avg reward: -59.48 Avg turns: 40.97\n",
      "----------------- Episode: 29 ----------------- \n",
      "simulation success rate 0.0, ave reward -59.45, ave turns 40.9\n",
      "Train on : 61291\n",
      "- cur bellman err 4.6039, experience replay pool 61291\n",
      "Simulation success rate 0.0, Ave reward -59.45, Ave turns 40.9, Best success rate 0\n",
      "Progress: 30 / 100, Success rate: 0 / 30 Avg reward: -59.50 Avg turns: 41.00\n",
      "----------------- Episode: 30 ----------------- \n",
      "simulation success rate 0.0, ave reward -57.99, ave turns 37.98\n",
      "Train on : 63190\n",
      "- cur bellman err 4.4857, experience replay pool 63190\n",
      "Simulation success rate 0.0, Ave reward -57.99, Ave turns 37.98, Best success rate 0\n",
      "Progress: 31 / 100, Success rate: 0 / 31 Avg reward: -59.52 Avg turns: 41.03\n",
      "----------------- Episode: 31 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 65290\n",
      "- cur bellman err 4.4787, experience replay pool 65290\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 32 / 100, Success rate: 0 / 32 Avg reward: -59.53 Avg turns: 41.06\n",
      "----------------- Episode: 32 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 67390\n",
      "- cur bellman err 4.6482, experience replay pool 67390\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 33 / 100, Success rate: 0 / 33 Avg reward: -59.55 Avg turns: 41.09\n",
      "----------------- Episode: 33 ----------------- \n",
      "simulation success rate 0.01, ave reward -58.67, ave turns 41.74\n",
      "Train on : 69477\n",
      "- cur bellman err 4.3019, experience replay pool 69477\n",
      "Simulation success rate 0.01, Ave reward -58.67, Ave turns 41.74, Best success rate 0\n",
      "Progress: 34 / 100, Success rate: 0 / 34 Avg reward: -59.56 Avg turns: 41.12\n",
      "----------------- Episode: 34 ----------------- \n",
      "simulation success rate 0.0, ave reward -59.91, ave turns 41.82\n",
      "Train on : 71568\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c7d112675a73>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarm_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-9270d26611ac>\u001b[0m in \u001b[0;36mrun_episodes\u001b[0;34m(count, status)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#                 best_res['epoch'] = episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0magt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dengl11/CS234-Project/agent/dqn_agt_tf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, num_batches, show_every)\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay_pool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;31m# batch_struct = self.dqn.singleBatch(batch, {'gamma': self.gamma}, self.clone_dqn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_bellman_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dengl11/CS234-Project/alg/dqn_tf.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, batch_experience)\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m       \u001b[0mactions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         }\n\u001b[0;32m--> 177\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreg_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \u001b[0;31m# print(\"loss = {} | reg_loss = {}\".format(loss, reg_loss))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "performance_records = {}\n",
    "performance_records['success_rate'] = {}\n",
    "performance_records['ave_turns'] = {}\n",
    "performance_records['ave_reward'] = {}\n",
    "\n",
    "best_model = {}\n",
    "best_res = {'success_rate': 0, 'ave_reward':float('-inf'), 'ave_turns': float('inf'), 'epoch':0}\n",
    "\n",
    "curve = []\n",
    "losses = []\n",
    "agent.warm_start = 1\n",
    "run_episodes(100, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_learning_curve(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_loss_curve(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "draw_loss_curve(losses[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# saver.save(agent.model.sess, \"trained_model/tf_400/\", global_step = 400)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
