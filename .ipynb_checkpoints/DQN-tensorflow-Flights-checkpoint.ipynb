{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of DQN in Tensorflow\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "from util import *\n",
    "from dlg_manager import *\n",
    "from alg import *\n",
    "from agent import *\n",
    "from user_sim import *\n",
    "from state_tracker import *\n",
    "import random\n",
    "from config import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nlg import *\n",
    "from six.moves import cPickle as pickle\n",
    "import IPython\n",
    "import copy, argparse, json\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 11\n",
      "Sample of dict:\n",
      "- inform: 1\n",
      "- not_sure: 10\n",
      "- request: 0\n",
      "- multiple_choice: 6\n",
      "- confirm_answer: 3\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "act_set_path = './data/dia_acts.txt'\n",
    "act_set = text_to_dict(act_set_path)\n",
    "sample_dict(act_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slot set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 34\n",
      "Sample of dict:\n",
      "- zip: 30\n",
      "- travelers: 4\n",
      "- numberofkids: 20\n",
      "- mc_list: 33\n",
      "- critic_rating: 9\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "slots_set_path = \"./parser/slot_set.txt\"\n",
    "slot_set = text_to_dict(slots_set_path)\n",
    "sample_dict(slot_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flight dic: info about flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 1000\n",
      "Sample of dict:\n",
      "- 72: {'flightDate1': '0', 'travelers': '2', 'flightDate2': '3', 'origin1': 'SEA', 'destination1': 'MCO'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "flight_kb_path = \"./parser/fkb.json.p\"\n",
    "flight_kb = pickle.load(open(flight_kb_path, 'rb'), encoding=\"latin\")\n",
    "sample_dict(flight_kb, sample_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generator (pretrained)\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "nlg_model_path = './data/trained_model/nlg/lstm_tanh_relu_[1468202263.38]_2_0.610.p'\n",
    "nlg_model = Nlg()\n",
    "nlg_model.load_nlg_model(nlg_model_path)\n",
    "diaact_nl_pairs_path = \"./parser/flight.nl.pairs.json\"\n",
    "nlg_model.load_predefine_act_nl_pairs(diaact_nl_pairs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grad_clip: -0.0001\n",
      "dia_slot_val: 2\n",
      "reg_cost: 0.001\n",
      "data_path: .\\data\\movieMultiLine.Annot.Corrected.Final.v3.csv\n",
      "save_check_point: 20\n",
      "slot_rep: 1\n",
      "max_epochs: 200\n",
      "sdgtype: rmsprop\n",
      "init_rnn: 0\n",
      "cv_fold: 6\n",
      "write_model_dir: .\\checkpoints\\template\\07102016\\\n",
      "valid_test: 0\n",
      "pretrained_model_path: None\n",
      "check_point: 20\n",
      "decay_rate: 0.999\n",
      "feed_recurrence: 0\n",
      "hidden_size: 100\n",
      "activation_func: relu\n",
      "momentum: 0.1\n",
      "learning_rate: 0.001\n",
      "batch_size: 16\n",
      "act_set: data/dia_acts.txt\n",
      "smooth_eps: 1e-08\n",
      "split_method: 1\n",
      "slot_set: data/slot_set.txt\n",
      "eva_metric: 2\n",
      "model: lstm_tanh\n",
      "trained_model_path: None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "model_params = pickle.load(open(nlg_model_path, 'rb'), encoding='latin1')\n",
    "params = model_params['params']\n",
    "params['batch_size'] = 16\n",
    "batch_size = 16\n",
    "save_check_point = 20\n",
    "params['trained_model_path'] = None\n",
    "for k in params:\n",
    "    print(\"{}: {}\".format(k, params[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Simulator\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goals length: 128\n",
      "Sample the first goal: \n",
      "{'request_slots': {}, 'diaact': 'request', 'inform_slots': {'flightDate1': '2', 'travelers': '4', 'flightDate2': '3', 'origin1': 'MUC', 'destination1': 'MCO'}}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "goal_file_path = './parser/fg.json.p'\n",
    "all_goal_set = pickle.load(open(goal_file_path, 'rb'), encoding=\"latin\")\n",
    "print(\"goals length: {}\".format(len(all_goal_set)))\n",
    "print(\"Sample the first goal: \\n{}\".format(all_goal_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split goal set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "0\n",
      "26\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# split goal set\n",
    "split_fold = params.get('split_fold', 5)\n",
    "goal_set = {'train':[], 'valid':[], 'test':[], 'all':[]}\n",
    "for u_goal_id, u_goal in enumerate(all_goal_set):\n",
    "    if u_goal_id % split_fold == 1: goal_set['test'].append(u_goal)\n",
    "    else: goal_set['train'].append(u_goal)\n",
    "    goal_set['all'].append(u_goal)\n",
    "print(len(goal_set['train']))\n",
    "print(len(goal_set['valid']))\n",
    "print(len(goal_set['test']))\n",
    "print(len(goal_set['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user simulator param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "usersim_params = {}\n",
    "usersim_params['max_turn'] = 40\n",
    "usersim_params['slot_err_prob'] = 0.00\n",
    "# slot_err_mode: 0 for slot_val only; 1 for three errs\n",
    "usersim_params['slot_err_mode'] = 0\n",
    "usersim_params['intent_err_prob'] = 0\n",
    "# run_mode: 0 for default NL; 1 for dia_act; 2 for both\n",
    "usersim_params['run_mode'] = 0\n",
    "# 0 for dia_act level; 1 for NL level\n",
    "usersim_params['act_level'] = 0\n",
    "# train/test/all; default is all\n",
    "usersim_params['learn_phase'] = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a flights dictionary for user simulator - slot:possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 20\n",
      "Sample of dict:\n",
      "- numberofpeople: ['2', '5', 'two', '9', 'three', '4', '3', '6', '1', 'four', '2 adult', 'one', '7', ' 2', 'single', '8']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "flight_dict_path = './parser/dicts.v3.p'\n",
    "flight_dictionary = pickle.load(open(flight_dict_path, 'rb'), encoding=\"latin\")\n",
    "samples = sample_dict(flight_dictionary, sample_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = RuleSimulator(flight_dictionary, act_set, slot_set, goal_set, usersim_params)\n",
    "# user = AlternateSimulator(flight_dictionary, act_set, slot_set, goal_set, usersim_params)\n",
    "user.set_nlg_model(nlg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained model path = None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "agent_params = {}\n",
    "# maximum length of each dialog (default=20, 0=no maximum length)\n",
    "agent_params['max_turn'] = 40\n",
    "# Epsilon to determine stochasticity of epsilon-greedy agent policies\n",
    "agent_params['epsilon'] = 0\n",
    "# run_mode: 0 for default NL; 1 for dia_act; 2 for both\n",
    "agent_params['agent_run_mode'] = 3\n",
    "# 0 for dia_act level; 1 for NL level\n",
    "agent_params['agent_act_level'] = 0\n",
    "\n",
    "############### DQN #################\n",
    "# the size for experience replay\n",
    "agent_params['experience_replay_pool_size'] = 10000\n",
    "# # the hidden size for DQN\n",
    "agent_params['dqn_hidden_size'] = 60\n",
    "agent_params['batch_size'] = 16\n",
    "# # gamma for DQN\n",
    "agent_params['gamma'] = 0.9\n",
    "# # predict model for DQN\n",
    "agent_params['predict_mode'] = True\n",
    "agent_params['trained_model_path'] = params['pretrained_model_path']\n",
    "#####################################\n",
    "print(\"pretrained model path = {}\".format(agent_params['trained_model_path']))\n",
    "# 0: no warm start; 1: warm start for training\n",
    "agent_params['warm_start'] = 1\n",
    "# run_mode: 0 for NL; 1 for dia_act\n",
    "agent_params['cmd_input_mode'] = 0\n",
    "\n",
    "success_rate_threshold = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n"
     ]
    }
   ],
   "source": [
    "# agent = RequestBasicsAgent(movie_kb, act_set, slot_set, agent_params)\n",
    "# agent = AgentDQN(movie_kb, act_set, slot_set, agent_params)\n",
    "# agt = 9\n",
    "agt = 10\n",
    "agent_params['batch_size']  = batch_size\n",
    "if agt == 9:\n",
    "    agent = AgentDQN(flight_kb, act_set, slot_set, agent_params)\n",
    "else:\n",
    "    agent = DQNAgentTF(flight_kb, act_set, slot_set, agent_params)\n",
    "\n",
    "agent.set_nlg_model(nlg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog Manager\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dlg_manager = DlgManager(agent, user, act_set, slot_set, flight_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Episodes\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "status = {'successes': 0, 'count': 0, 'cumulative_reward': 0}\n",
    "# the size of validation set\n",
    "simulation_epoch_size = 100\n",
    "# the number of epochs for warm start \n",
    "warm_start_epochs = 200\n",
    "# num_episodes = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Warm_Start Simulation (by Rule Policy) \"\"\"\n",
    "def warm_start_simulation():\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    res = {}\n",
    "    for episode in range(warm_start_epochs):\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "            if episode_over:\n",
    "                if reward > 0: \n",
    "                    successes += 1\n",
    "#                     print (\"warm_start simulation episode %s: Success\" % (episode))\n",
    "#                 else: print (\"warm_start simulation episode %s: Fail\" % (episode))\n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "        \n",
    "        if len(agent.experience_replay_pool) >= agent.experience_replay_pool_size:\n",
    "            break\n",
    "    \n",
    "    agent.warm_start = 2\n",
    "    res['success_rate'] = float(successes)/simulation_epoch_size\n",
    "    res['ave_reward'] = float(cumulative_reward)/simulation_epoch_size\n",
    "    res['ave_turns'] = float(cumulative_turns)/simulation_epoch_size\n",
    "    print (\"Warm_Start %s epochs, success rate %s, ave reward %s, ave turns %s\" % (episode+1, res['success_rate'], res['ave_reward'], res['ave_turns']))\n",
    "    print (\"Current experience replay buffer size %s\" % (len(agent.experience_replay_pool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulation_epoch(simulation_epoch_size):\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    res = {}\n",
    "    for episode in range(simulation_epoch_size):\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "            if episode_over:\n",
    "                if reward > 0: \n",
    "                    successes += 1\n",
    "#                     print (\"simulation episode %s: Success\" % (episode))\n",
    "#                 else: print (\"simulation episode %s: Fail\" % (episode))\n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "    \n",
    "    res['success_rate'] = float(successes)/simulation_epoch_size\n",
    "    res['ave_reward'] = float(cumulative_reward)/simulation_epoch_size\n",
    "    res['ave_turns'] = float(cumulative_turns)/simulation_epoch_size\n",
    "    print(\"simulation success rate %s, ave reward %s, ave turns %s\" % (res['success_rate'], res['ave_reward'], res['ave_turns']))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def run_episodes(count, status):\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    \n",
    "    if agt >= 9 and params['trained_model_path'] == None and agent.warm_start == 1:\n",
    "        print ('warm_start starting ...')\n",
    "        warm_start_simulation()\n",
    "        print ('warm_start finished, start RL training ...')\n",
    "    \n",
    "    for episode in range(count):\n",
    "        print (\"----------------- Episode: %s ----------------- \" % (episode))\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        \n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "                \n",
    "            if episode_over:\n",
    "                if reward > 0:\n",
    "                    print (\"Successful Dialog!\")\n",
    "                    successes += 1\n",
    "#                 else: print (\"Failed Dialog!\")\n",
    "                \n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "        \n",
    "        # simulation\n",
    "        if agt >= 9 and params['trained_model_path'] == None:\n",
    "            agent.predict_mode = True\n",
    "            simulation_res = simulation_epoch(simulation_epoch_size)\n",
    "            \n",
    "            performance_records['success_rate'][episode] = simulation_res['success_rate']\n",
    "            performance_records['ave_turns'][episode] = simulation_res['ave_turns']\n",
    "            performance_records['ave_reward'][episode] = simulation_res['ave_reward']\n",
    "            \n",
    "            if simulation_res['success_rate'] >= best_res['success_rate']:\n",
    "                if simulation_res['success_rate'] >= success_rate_threshold: # threshold = 0.30\n",
    "                    agent.experience_replay_pool = [] \n",
    "                    simulation_epoch(simulation_epoch_size)\n",
    "                \n",
    "#             if simulation_res['success_rate'] > best_res['success_rate']:\n",
    "#                 best_model['model'] = copy.deepcopy(agent)\n",
    "#                 best_res['success_rate'] = simulation_res['success_rate']\n",
    "#                 best_res['ave_reward'] = simulation_res['ave_reward']\n",
    "#                 best_res['ave_turns'] = simulation_res['ave_turns']\n",
    "#                 best_res['epoch'] = episode\n",
    "                \n",
    "            loss = agent.train(batch_size, 1)\n",
    "            if agt == 10: \n",
    "                agent.model.update_target_params()\n",
    "            else: \n",
    "                agent.clone_dqn = copy.deepcopy(agent.dqn)\n",
    "                \n",
    "            agent.predict_mode = False\n",
    "            \n",
    "            print (\"Simulation success rate %s, Ave reward %s, Ave turns %s, Best success rate %s\" % (performance_records['success_rate'][episode], performance_records['ave_reward'][episode], performance_records['ave_turns'][episode], best_res['success_rate']))\n",
    "#             if episode % save_check_point == 0 and params['trained_model_path'] == None: # save the model every 10 episodes\n",
    "#                 save_model(params['write_model_dir'], agt, best_res['success_rate'], best_model['model'], best_res['epoch'], episode)\n",
    "#                 save_performance_records(params['write_model_dir'], agt, performance_records)\n",
    "        curve.append(successes/(episode+1))\n",
    "        losses.append(loss)\n",
    "        print(\"Progress: %s / %s, Success rate: %s / %s Avg reward: %.2f Avg turns: %.2f\" % (episode+1, count, successes, episode+1, float(cumulative_reward)/(episode+1), float(cumulative_turns)/(episode+1)))\n",
    "    print(\"Success rate: %s / %s Avg reward: %.2f Avg turns: %.2f\" % (successes, count, float(cumulative_reward)/count, float(cumulative_turns)/count))\n",
    "    status['successes'] += successes\n",
    "    status['count'] += count\n",
    "    \n",
    "#     if agt == 9 and params['traained_model_path'] == None:\n",
    "#         save_model(params['write_model_dir'], agt, float(successes)/count, best_model['model'], best_res['epoch'], count)\n",
    "#         save_performance_records(params['write_model_dir'], agt, performance_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Warm Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Eval\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------- Episode: 0 ----------------- \n",
      "simulation success rate 0.0, ave reward -46.7, ave turns 15.4\n",
      "Train on : 772\n",
      "- cur bellman err 13.1528, experience replay pool 772\n",
      "Simulation success rate 0.0, Ave reward -46.7, Ave turns 15.4, Best success rate 0\n",
      "Progress: 1 / 100, Success rate: 0 / 1 Avg reward: -41.00 Avg turns: 4.00\n",
      "----------------- Episode: 1 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 2872\n",
      "- cur bellman err 4.2840, experience replay pool 2872\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 2 / 100, Success rate: 0 / 2 Avg reward: -50.50 Avg turns: 23.00\n",
      "----------------- Episode: 2 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 4972\n",
      "- cur bellman err 3.6963, experience replay pool 4972\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 3 / 100, Success rate: 0 / 3 Avg reward: -53.67 Avg turns: 29.33\n",
      "----------------- Episode: 3 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 7072\n",
      "- cur bellman err 2.8350, experience replay pool 7072\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 4 / 100, Success rate: 0 / 4 Avg reward: -55.25 Avg turns: 32.50\n",
      "----------------- Episode: 4 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 9172\n",
      "- cur bellman err 1.0080, experience replay pool 9172\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 5 / 100, Success rate: 0 / 5 Avg reward: -56.20 Avg turns: 34.40\n",
      "----------------- Episode: 5 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 11272\n",
      "- cur bellman err 0.4566, experience replay pool 11272\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 6 / 100, Success rate: 0 / 6 Avg reward: -56.83 Avg turns: 35.67\n",
      "----------------- Episode: 6 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 13372\n",
      "- cur bellman err 0.3006, experience replay pool 13372\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 7 / 100, Success rate: 0 / 7 Avg reward: -57.29 Avg turns: 36.57\n",
      "----------------- Episode: 7 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 15472\n",
      "- cur bellman err 0.1315, experience replay pool 15472\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 8 / 100, Success rate: 0 / 8 Avg reward: -57.62 Avg turns: 37.25\n",
      "----------------- Episode: 8 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 17572\n",
      "- cur bellman err 0.0776, experience replay pool 17572\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 9 / 100, Success rate: 0 / 9 Avg reward: -57.89 Avg turns: 37.78\n",
      "----------------- Episode: 9 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 19672\n",
      "- cur bellman err 0.1135, experience replay pool 19672\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 10 / 100, Success rate: 0 / 10 Avg reward: -58.10 Avg turns: 38.20\n",
      "----------------- Episode: 10 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 21772\n",
      "- cur bellman err 0.0599, experience replay pool 21772\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 11 / 100, Success rate: 0 / 11 Avg reward: -58.27 Avg turns: 38.55\n",
      "----------------- Episode: 11 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 23872\n",
      "- cur bellman err 0.0687, experience replay pool 23872\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 12 / 100, Success rate: 0 / 12 Avg reward: -58.42 Avg turns: 38.83\n",
      "----------------- Episode: 12 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 25972\n",
      "- cur bellman err 0.0634, experience replay pool 25972\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 13 / 100, Success rate: 0 / 13 Avg reward: -58.54 Avg turns: 39.08\n",
      "----------------- Episode: 13 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 28072\n",
      "- cur bellman err 0.0516, experience replay pool 28072\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 14 / 100, Success rate: 0 / 14 Avg reward: -58.64 Avg turns: 39.29\n",
      "----------------- Episode: 14 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 30172\n",
      "- cur bellman err 0.0542, experience replay pool 30172\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 15 / 100, Success rate: 0 / 15 Avg reward: -58.73 Avg turns: 39.47\n",
      "----------------- Episode: 15 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 32272\n",
      "- cur bellman err 0.0417, experience replay pool 32272\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 16 / 100, Success rate: 0 / 16 Avg reward: -58.81 Avg turns: 39.62\n",
      "----------------- Episode: 16 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 34372\n",
      "- cur bellman err 0.0400, experience replay pool 34372\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 17 / 100, Success rate: 0 / 17 Avg reward: -58.88 Avg turns: 39.76\n",
      "----------------- Episode: 17 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 36472\n",
      "- cur bellman err 0.0328, experience replay pool 36472\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 18 / 100, Success rate: 0 / 18 Avg reward: -58.94 Avg turns: 39.89\n",
      "----------------- Episode: 18 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 38572\n",
      "- cur bellman err 0.0340, experience replay pool 38572\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 19 / 100, Success rate: 0 / 19 Avg reward: -59.00 Avg turns: 40.00\n",
      "----------------- Episode: 19 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 40672\n",
      "- cur bellman err 0.0189, experience replay pool 40672\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 20 / 100, Success rate: 0 / 20 Avg reward: -59.05 Avg turns: 40.10\n",
      "----------------- Episode: 20 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 42772\n",
      "- cur bellman err 0.0206, experience replay pool 42772\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 21 / 100, Success rate: 0 / 21 Avg reward: -59.10 Avg turns: 40.19\n",
      "----------------- Episode: 21 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 44872\n",
      "- cur bellman err 0.0202, experience replay pool 44872\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 22 / 100, Success rate: 0 / 22 Avg reward: -59.14 Avg turns: 40.27\n",
      "----------------- Episode: 22 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 46972\n",
      "- cur bellman err 0.0169, experience replay pool 46972\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 23 / 100, Success rate: 0 / 23 Avg reward: -59.17 Avg turns: 40.35\n",
      "----------------- Episode: 23 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 49072\n",
      "- cur bellman err 0.0115, experience replay pool 49072\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 24 / 100, Success rate: 0 / 24 Avg reward: -59.21 Avg turns: 40.42\n",
      "----------------- Episode: 24 ----------------- \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 51172\n",
      "- cur bellman err 0.1786, experience replay pool 51172\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 25 / 100, Success rate: 0 / 25 Avg reward: -59.24 Avg turns: 40.48\n",
      "----------------- Episode: 25 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 53272\n",
      "- cur bellman err 0.0389, experience replay pool 53272\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 26 / 100, Success rate: 0 / 26 Avg reward: -59.27 Avg turns: 40.54\n",
      "----------------- Episode: 26 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 55372\n",
      "- cur bellman err 0.0170, experience replay pool 55372\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 27 / 100, Success rate: 0 / 27 Avg reward: -59.30 Avg turns: 40.59\n",
      "----------------- Episode: 27 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 57472\n",
      "- cur bellman err 0.0107, experience replay pool 57472\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 28 / 100, Success rate: 0 / 28 Avg reward: -59.32 Avg turns: 40.64\n",
      "----------------- Episode: 28 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 59572\n",
      "- cur bellman err 0.0092, experience replay pool 59572\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 29 / 100, Success rate: 0 / 29 Avg reward: -59.34 Avg turns: 40.69\n",
      "----------------- Episode: 29 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 61672\n",
      "- cur bellman err 0.0081, experience replay pool 61672\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 30 / 100, Success rate: 0 / 30 Avg reward: -59.37 Avg turns: 40.73\n",
      "----------------- Episode: 30 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 63772\n",
      "- cur bellman err 0.0072, experience replay pool 63772\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 31 / 100, Success rate: 0 / 31 Avg reward: -59.39 Avg turns: 40.77\n",
      "----------------- Episode: 31 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 65872\n",
      "- cur bellman err 0.0068, experience replay pool 65872\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 32 / 100, Success rate: 0 / 32 Avg reward: -59.41 Avg turns: 40.81\n",
      "----------------- Episode: 32 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 67972\n",
      "- cur bellman err 0.0065, experience replay pool 67972\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 33 / 100, Success rate: 0 / 33 Avg reward: -59.42 Avg turns: 40.85\n",
      "----------------- Episode: 33 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 70072\n",
      "- cur bellman err 0.0072, experience replay pool 70072\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 34 / 100, Success rate: 0 / 34 Avg reward: -59.44 Avg turns: 40.88\n",
      "----------------- Episode: 34 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 72172\n",
      "- cur bellman err 0.0056, experience replay pool 72172\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 35 / 100, Success rate: 0 / 35 Avg reward: -59.46 Avg turns: 40.91\n",
      "----------------- Episode: 35 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 74272\n",
      "- cur bellman err 0.0055, experience replay pool 74272\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 36 / 100, Success rate: 0 / 36 Avg reward: -59.47 Avg turns: 40.94\n",
      "----------------- Episode: 36 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 76372\n",
      "- cur bellman err 0.0048, experience replay pool 76372\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 37 / 100, Success rate: 0 / 37 Avg reward: -59.49 Avg turns: 40.97\n",
      "----------------- Episode: 37 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 78472\n",
      "- cur bellman err 0.0042, experience replay pool 78472\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 38 / 100, Success rate: 0 / 38 Avg reward: -59.50 Avg turns: 41.00\n",
      "----------------- Episode: 38 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 80572\n",
      "- cur bellman err 0.0036, experience replay pool 80572\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 39 / 100, Success rate: 0 / 39 Avg reward: -59.51 Avg turns: 41.03\n",
      "----------------- Episode: 39 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 82672\n",
      "- cur bellman err 0.0032, experience replay pool 82672\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 40 / 100, Success rate: 0 / 40 Avg reward: -59.52 Avg turns: 41.05\n",
      "----------------- Episode: 40 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 84772\n",
      "- cur bellman err 0.0027, experience replay pool 84772\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 41 / 100, Success rate: 0 / 41 Avg reward: -59.54 Avg turns: 41.07\n",
      "----------------- Episode: 41 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 86872\n",
      "- cur bellman err 0.0027, experience replay pool 86872\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 42 / 100, Success rate: 0 / 42 Avg reward: -59.55 Avg turns: 41.10\n",
      "----------------- Episode: 42 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 88972\n",
      "- cur bellman err 0.0024, experience replay pool 88972\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 43 / 100, Success rate: 0 / 43 Avg reward: -59.56 Avg turns: 41.12\n",
      "----------------- Episode: 43 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 91072\n",
      "- cur bellman err 0.0022, experience replay pool 91072\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 44 / 100, Success rate: 0 / 44 Avg reward: -59.57 Avg turns: 41.14\n",
      "----------------- Episode: 44 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 93172\n",
      "- cur bellman err 0.0020, experience replay pool 93172\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 45 / 100, Success rate: 0 / 45 Avg reward: -59.58 Avg turns: 41.16\n",
      "----------------- Episode: 45 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 95272\n",
      "- cur bellman err 0.0018, experience replay pool 95272\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 46 / 100, Success rate: 0 / 46 Avg reward: -59.59 Avg turns: 41.17\n",
      "----------------- Episode: 46 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 97372\n",
      "- cur bellman err 0.0017, experience replay pool 97372\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 47 / 100, Success rate: 0 / 47 Avg reward: -59.60 Avg turns: 41.19\n",
      "----------------- Episode: 47 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 99472\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-382547d7a910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarm_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mrun_episodes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-35-9270d26611ac>\u001b[0m in \u001b[0;36mrun_episodes\u001b[0;34m(count, status)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;31m#                 best_res['epoch'] = episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0magt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dengl11/Dropbox/1_2017_Spring/CS_234/_CS234_Project_/Github/agent/dqn_agt_tf.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, batch_size, num_batches, show_every)\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0miter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay_pool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperience_replay_pool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m                 \u001b[0;31m# batch_struct = self.dqn.singleBatch(batch, {'gamma': self.gamma}, self.clone_dqn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcur_bellman_err\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/dengl11/Dropbox/1_2017_Spring/CS_234/_CS234_Project_/Github/alg/dqn_tf.py\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, batch_experience)\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mcurr_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_experience\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, 252]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mnext_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_experience\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, 252]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mdone\u001b[0m        \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_experience\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, 252]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m         \u001b[0mrewards\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_experience\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, 252]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mactions\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch_experience\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# [batch_size, 252]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "performance_records = {}\n",
    "performance_records['success_rate'] = {}\n",
    "performance_records['ave_turns'] = {}\n",
    "performance_records['ave_reward'] = {}\n",
    "\n",
    "best_model = {}\n",
    "best_res = {'success_rate': 0, 'ave_reward':float('-inf'), 'ave_turns': float('inf'), 'epoch':0}\n",
    "\n",
    "curve = []\n",
    "losses = []\n",
    "agent.warm_start = 0\n",
    "run_episodes(100, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_learning_curve(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_curve(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_curve(losses[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(agent.model.sess, \"trained_model/tf_400/\", global_step = 400)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
