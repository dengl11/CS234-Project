{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training of DQN in Tensorflow\n",
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "from util import *\n",
    "from dlg_manager import *\n",
    "from alg import *\n",
    "from agent import *\n",
    "from user_sim import *\n",
    "from state_tracker import *\n",
    "import random\n",
    "from config import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from nlg import *\n",
    "from six.moves import cPickle as pickle\n",
    "import IPython\n",
    "import copy, argparse, json\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Action Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 11\n",
      "Sample of dict:\n",
      "- confirm_answer: 3\n",
      "- inform: 1\n",
      "- confirm_question: 2\n",
      "- welcome: 8\n",
      "- not_sure: 10\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "act_set_path = './data/dia_acts.txt'\n",
    "act_set = text_to_dict(act_set_path)\n",
    "sample_dict(act_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slot set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 7\n",
      "Sample of dict:\n",
      "- flightDate2: 1\n",
      "- taskcomplete: 6\n",
      "- flightDate1: 2\n",
      "- destination1: 0\n",
      "- ticket: 5\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "slots_set_path = \"./parser/slot_set.txt\"\n",
    "slot_set = text_to_dict(slots_set_path)\n",
    "sample_dict(slot_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### flight dic: info about flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 1000\n",
      "Sample of dict:\n",
      "- 342: {'flightDate2': '3', 'origin1': 'CTU', 'travelers': '4', 'destination1': 'MCO', 'flightDate1': '1'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "flight_kb_path = \"./parser/fkb.json.p\"\n",
    "flight_kb = pickle.load(open(flight_kb_path, 'rb'), encoding=\"latin\")\n",
    "sample_dict(flight_kb, sample_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Generator (pretrained)\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "nlg_model_path ='data/trained_model/nlg/lstm_tanh_relu_[1468202263.38]_2_0.610.p'\n",
    "nlg_model = Nlg()\n",
    "nlg_model.load_nlg_model(nlg_model_path)\n",
    "diaact_nl_pairs_path = \"./parser/flight.nl.pairs.json\"\n",
    "nlg_model.load_predefine_act_nl_pairs(diaact_nl_pairs_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save_check_point: 20\n",
      "valid_test: 0\n",
      "slot_set: data/slot_set.txt\n",
      "slot_rep: 1\n",
      "reg_cost: 0.001\n",
      "max_epochs: 200\n",
      "hidden_size: 100\n",
      "init_rnn: 0\n",
      "sdgtype: rmsprop\n",
      "split_method: 1\n",
      "pretrained_model_path: None\n",
      "activation_func: relu\n",
      "feed_recurrence: 0\n",
      "check_point: 20\n",
      "trained_model_path: None\n",
      "dia_slot_val: 2\n",
      "grad_clip: -0.0001\n",
      "smooth_eps: 1e-08\n",
      "batch_size: 16\n",
      "cv_fold: 6\n",
      "act_set: data/dia_acts.txt\n",
      "learning_rate: 0.001\n",
      "model: lstm_tanh\n",
      "data_path: .\\data\\movieMultiLine.Annot.Corrected.Final.v3.csv\n",
      "write_model_dir: .\\checkpoints\\template\\07102016\\\n",
      "decay_rate: 0.999\n",
      "eva_metric: 2\n",
      "momentum: 0.1\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "model_params = pickle.load(open(nlg_model_path, 'rb'), encoding='latin1')\n",
    "params = model_params['params']\n",
    "params['batch_size'] = 16\n",
    "batch_size = 16\n",
    "save_check_point = 20\n",
    "params['trained_model_path'] = None\n",
    "for k in params:\n",
    "    print(\"{}: {}\".format(k, params[k]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User Simulator\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### goal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": [],
    "init_cell": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "goals length: 128\n",
      "Sample the first goal: \n",
      "{'request_slots': {}, 'inform_slots': {'flightDate2': '3', 'origin1': 'MUC', 'travelers': '4', 'destination1': 'MCO', 'flightDate1': '2'}, 'diaact': 'request'}\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "goal_file_path = './parser/fg.json.p'\n",
    "all_goal_set = pickle.load(open(goal_file_path, 'rb'), encoding=\"latin\")\n",
    "print(\"goals length: {}\".format(len(all_goal_set)))\n",
    "print(\"Sample the first goal: \\n{}\".format(all_goal_set[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split goal set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102\n",
      "0\n",
      "26\n",
      "128\n"
     ]
    }
   ],
   "source": [
    "# split goal set\n",
    "split_fold = params.get('split_fold', 5)\n",
    "goal_set = {'train':[], 'valid':[], 'test':[], 'all':[]}\n",
    "for u_goal_id, u_goal in enumerate(all_goal_set):\n",
    "    if u_goal_id % split_fold == 1: goal_set['test'].append(u_goal)\n",
    "    else: goal_set['train'].append(u_goal)\n",
    "    goal_set['all'].append(u_goal)\n",
    "print(len(goal_set['train']))\n",
    "print(len(goal_set['valid']))\n",
    "print(len(goal_set['test']))\n",
    "print(len(goal_set['all']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### user simulator param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "usersim_params = {}\n",
    "usersim_params['max_turn'] = 40\n",
    "usersim_params['slot_err_prob'] = 0.00\n",
    "# slot_err_mode: 0 for slot_val only; 1 for three errs\n",
    "usersim_params['slot_err_mode'] = 0\n",
    "usersim_params['intent_err_prob'] = 0\n",
    "# run_mode: 0 for default NL; 1 for dia_act; 2 for both\n",
    "usersim_params['run_mode'] = 0\n",
    "# 0 for dia_act level; 1 for NL level\n",
    "usersim_params['act_level'] = 0\n",
    "# train/test/all; default is all\n",
    "usersim_params['learn_phase'] = 'all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a flights dictionary for user simulator - slot:possible values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys = 20\n",
      "Sample of dict:\n",
      "- distanceconstraints: ['closest', 'visalia', 'near the space needle', 'area', 'nearest', 'near 98119', 'closest time', 'space needle', 'east side', 'downtown', 'close to 95833', 'near space needle', 'south beach', 'near', 'south barrington', 'near me', 'south side', 'local theater', 'closest theater to you', '12 miles', ' seattle area', 'northern part of the city', 'near here', 'near my location', 'safeco field', 'near you:', 'north side', 'vicinity', 'around the city', 'far away from disney', 'near you', 'near safeco field', 'in your area', 'your area', 'ave', 'general']\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "flight_dict_path = './parser/dicts.v3.p'\n",
    "flight_dictionary = pickle.load(open(flight_dict_path, 'rb'), encoding=\"latin\")\n",
    "samples = sample_dict(flight_dictionary, sample_size=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Create a User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user = RuleSimulator(flight_dictionary, act_set, slot_set, goal_set, usersim_params)\n",
    "# user = AlternateSimulator(flight_dictionary, act_set, slot_set, goal_set, usersim_params)\n",
    "user.set_nlg_model(nlg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pretrained model path = None\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "agent_params = {}\n",
    "# maximum length of each dialog (default=20, 0=no maximum length)\n",
    "agent_params['max_turn'] = 40\n",
    "# Epsilon to determine stochasticity of epsilon-greedy agent policies\n",
    "agent_params['epsilon'] = 0\n",
    "# run_mode: 0 for default NL; 1 for dia_act; 2 for both\n",
    "agent_params['agent_run_mode'] = 3\n",
    "# 0 for dia_act level; 1 for NL level\n",
    "agent_params['agent_act_level'] = 0\n",
    "\n",
    "############### DQN #################\n",
    "# the size for experience replay\n",
    "agent_params['experience_replay_pool_size'] = 10000\n",
    "# # the hidden size for DQN\n",
    "agent_params['dqn_hidden_size'] = 60\n",
    "agent_params['batch_size'] = 16\n",
    "# # gamma for DQN\n",
    "agent_params['gamma'] = 0.9\n",
    "# # predict model for DQN\n",
    "agent_params['predict_mode'] = True\n",
    "agent_params['trained_model_path'] = params['pretrained_model_path']\n",
    "#####################################\n",
    "print(\"pretrained model path = {}\".format(agent_params['trained_model_path']))\n",
    "# 0: no warm start; 1: warm start for training\n",
    "agent_params['warm_start'] = 1\n",
    "# run_mode: 0 for NL; 1 for dia_act\n",
    "agent_params['cmd_input_mode'] = 0\n",
    "\n",
    "success_rate_threshold = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create an agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "act_cardinality  11\n",
      "feasible_actions 16\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "# agent = RequestBasicsAgent(movie_kb, act_set, slot_set, agent_params)\n",
    "# agent = AgentDQN(movie_kb, act_set, slot_set, agent_params)\n",
    "# agt = 9\n",
    "agt = 10\n",
    "agent_params['batch_size']  = batch_size\n",
    "if agt == 9:\n",
    "    agent = AgentDQN(flight_kb, act_set, slot_set, agent_params)\n",
    "else:\n",
    "    agent = DQNAgentTF(flight_kb, act_set, slot_set, agent_params, transfer=False, path=\"trained_model/tf_100/model.ckpt\")\n",
    "\n",
    "agent.set_nlg_model(nlg_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dialog Manager\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dlg_manager = DlgManager(agent, user, act_set, slot_set, flight_kb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Episodes\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "code_folding": [],
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "status = {'successes': 0, 'count': 0, 'cumulative_reward': 0}\n",
    "# the size of validation set\n",
    "simulation_epoch_size = 100\n",
    "# the number of epochs for warm start \n",
    "warm_start_epochs = 100\n",
    "# num_episodes = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\" Warm_Start Simulation (by Rule Policy) \"\"\"\n",
    "def warm_start_simulation():\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    res = {}\n",
    "    for episode in range(warm_start_epochs):\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "            if episode_over:\n",
    "                if reward > 0: \n",
    "                    successes += 1\n",
    "#                     print (\"warm_start simulation episode %s: Success\" % (episode))\n",
    "#                 else: print (\"warm_start simulation episode %s: Fail\" % (episode))\n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "        \n",
    "        if len(agent.experience_replay_pool) >= agent.experience_replay_pool_size:\n",
    "            break\n",
    "    \n",
    "    agent.warm_start = 2\n",
    "    res['success_rate'] = float(successes)/simulation_epoch_size\n",
    "    res['ave_reward'] = float(cumulative_reward)/simulation_epoch_size\n",
    "    res['ave_turns'] = float(cumulative_turns)/simulation_epoch_size\n",
    "    print (\"Warm_Start %s epochs, success rate %s, ave reward %s, ave turns %s\" % (episode+1, res['success_rate'], res['ave_reward'], res['ave_turns']))\n",
    "    print (\"Current experience replay buffer size %s\" % (len(agent.experience_replay_pool)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def simulation_epoch(simulation_epoch_size):\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    res = {}\n",
    "    for episode in range(simulation_epoch_size):\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "            if episode_over:\n",
    "                if reward > 0: \n",
    "                    successes += 1\n",
    "#                     print (\"simulation episode %s: Success\" % (episode))\n",
    "#                 else: print (\"simulation episode %s: Fail\" % (episode))\n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "    \n",
    "    res['success_rate'] = float(successes)/simulation_epoch_size\n",
    "    res['ave_reward'] = float(cumulative_reward)/simulation_epoch_size\n",
    "    res['ave_turns'] = float(cumulative_turns)/simulation_epoch_size\n",
    "    print(\"simulation success rate %s, ave reward %s, ave turns %s\" % (res['success_rate'], res['ave_reward'], res['ave_turns']))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "collapsed": true,
    "init_cell": true
   },
   "outputs": [],
   "source": [
    "def run_episodes(count, status):\n",
    "    successes = 0\n",
    "    cumulative_reward = 0\n",
    "    cumulative_turns = 0\n",
    "    \n",
    "    \n",
    "    if agt >= 9 and params['trained_model_path'] == None and agent.warm_start == 1:\n",
    "        print ('warm_start starting ...')\n",
    "        warm_start_simulation()\n",
    "        print ('warm_start finished, start RL training ...')\n",
    "    \n",
    "    for episode in range(count):\n",
    "        print (\"----------------- Episode: %s ----------------- \" % (episode))\n",
    "        dlg_manager.init_episode()\n",
    "        episode_over = False\n",
    "        \n",
    "        while(not episode_over):\n",
    "            episode_over, reward = dlg_manager.step()\n",
    "            cumulative_reward += reward\n",
    "                \n",
    "            if episode_over:\n",
    "                if reward > 0:\n",
    "                    print (\"Successful Dialog!\")\n",
    "                    successes += 1\n",
    "#                 else: print (\"Failed Dialog!\")\n",
    "                \n",
    "                cumulative_turns += dlg_manager.state_tracker.turn_count\n",
    "        \n",
    "        # simulation\n",
    "        if agt >= 9 and params['trained_model_path'] == None:\n",
    "            agent.predict_mode = True\n",
    "            simulation_res = simulation_epoch(simulation_epoch_size)\n",
    "            \n",
    "            performance_records['success_rate'][episode] = simulation_res['success_rate']\n",
    "            performance_records['ave_turns'][episode] = simulation_res['ave_turns']\n",
    "            performance_records['ave_reward'][episode] = simulation_res['ave_reward']\n",
    "            \n",
    "            if simulation_res['success_rate'] >= best_res['success_rate']:\n",
    "                if simulation_res['success_rate'] >= success_rate_threshold: # threshold = 0.30\n",
    "                    agent.experience_replay_pool = [] \n",
    "                    simulation_epoch(simulation_epoch_size)\n",
    "                \n",
    "#             if simulation_res['success_rate'] > best_res['success_rate']:\n",
    "#                 best_model['model'] = copy.deepcopy(agent)\n",
    "#                 best_res['success_rate'] = simulation_res['success_rate']\n",
    "#                 best_res['ave_reward'] = simulation_res['ave_reward']\n",
    "#                 best_res['ave_turns'] = simulation_res['ave_turns']\n",
    "#                 best_res['epoch'] = episode\n",
    "                \n",
    "            loss = agent.train(batch_size, 1)\n",
    "            if agt == 10: \n",
    "                agent.model.update_target_params()\n",
    "            else: \n",
    "                agent.clone_dqn = copy.deepcopy(agent.dqn)\n",
    "                \n",
    "            agent.predict_mode = False\n",
    "            \n",
    "            print (\"Simulation success rate %s, Ave reward %s, Ave turns %s, Best success rate %s\" % (performance_records['success_rate'][episode], performance_records['ave_reward'][episode], performance_records['ave_turns'][episode], best_res['success_rate']))\n",
    "#             if episode % save_check_point == 0 and params['trained_model_path'] == None: # save the model every 10 episodes\n",
    "#                 save_model(params['write_model_dir'], agt, best_res['success_rate'], best_model['model'], best_res['epoch'], episode)\n",
    "#                 save_performance_records(params['write_model_dir'], agt, performance_records)\n",
    "        curve.append(successes/(episode+1))\n",
    "        losses.append(loss)\n",
    "        print(\"Progress: %s / %s, Success rate: %s / %s Avg reward: %.2f Avg turns: %.2f\" % (episode+1, count, successes, episode+1, float(cumulative_reward)/(episode+1), float(cumulative_turns)/(episode+1)))\n",
    "    print(\"Success rate: %s / %s Avg reward: %.2f Avg turns: %.2f\" % (successes, count, float(cumulative_reward)/count, float(cumulative_turns)/count))\n",
    "    status['successes'] += successes\n",
    "    status['count'] += count\n",
    "    \n",
    "#     if agt == 9 and params['traained_model_path'] == None:\n",
    "#         save_model(params['write_model_dir'], agt, float(successes)/count, best_model['model'], best_res['epoch'], count)\n",
    "#         save_performance_records(params['write_model_dir'], agt, performance_records)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run a Warm Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train & Eval\n",
    "-------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm_start starting ...\n",
      "Warm_Start 100 epochs, success rate 1.0, ave reward 74.0, ave turns 14.0\n",
      "Current experience replay buffer size 700\n",
      "warm_start finished, start RL training ...\n",
      "----------------- Episode: 0 ----------------- \n",
      "simulation success rate 0.0, ave reward -40.03, ave turns 2.06\n",
      "Train on : 804\n",
      "- cur bellman err 63.1122, experience replay pool 804\n",
      "Simulation success rate 0.0, Ave reward -40.03, Ave turns 2.06, Best success rate 0\n",
      "Progress: 1 / 100, Success rate: 0 / 1 Avg reward: -40.00 Avg turns: 2.00\n",
      "----------------- Episode: 1 ----------------- \n",
      "simulation success rate 0.0, ave reward -41.28, ave turns 4.56\n",
      "Train on : 1032\n",
      "- cur bellman err 49.2231, experience replay pool 1032\n",
      "Simulation success rate 0.0, Ave reward -41.28, Ave turns 4.56, Best success rate 0\n",
      "Progress: 2 / 100, Success rate: 0 / 2 Avg reward: -40.50 Avg turns: 3.00\n",
      "----------------- Episode: 2 ----------------- \n",
      "simulation success rate 0.0, ave reward -41.0, ave turns 4.0\n",
      "Train on : 1232\n",
      "- cur bellman err 39.1054, experience replay pool 1232\n",
      "Simulation success rate 0.0, Ave reward -41.0, Ave turns 4.0, Best success rate 0\n",
      "Progress: 3 / 100, Success rate: 0 / 3 Avg reward: -40.67 Avg turns: 3.33\n",
      "----------------- Episode: 3 ----------------- \n",
      "simulation success rate 0.0, ave reward -41.61, ave turns 5.22\n",
      "Train on : 1493\n",
      "- cur bellman err 27.9189, experience replay pool 1493\n",
      "Simulation success rate 0.0, Ave reward -41.61, Ave turns 5.22, Best success rate 0\n",
      "Progress: 4 / 100, Success rate: 0 / 4 Avg reward: -40.75 Avg turns: 3.50\n",
      "----------------- Episode: 4 ----------------- \n",
      "simulation success rate 0.0, ave reward -48.66, ave turns 19.32\n",
      "Train on : 2459\n",
      "- cur bellman err 15.5532, experience replay pool 2459\n",
      "Simulation success rate 0.0, Ave reward -48.66, Ave turns 19.32, Best success rate 0\n",
      "Progress: 5 / 100, Success rate: 0 / 5 Avg reward: -41.80 Avg turns: 5.60\n",
      "----------------- Episode: 5 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 4559\n",
      "- cur bellman err 7.5127, experience replay pool 4559\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 6 / 100, Success rate: 0 / 6 Avg reward: -44.83 Avg turns: 11.67\n",
      "----------------- Episode: 6 ----------------- \n",
      "simulation success rate 0.0, ave reward -56.64, ave turns 35.28\n",
      "Train on : 6323\n",
      "- cur bellman err 6.5226, experience replay pool 6323\n",
      "Simulation success rate 0.0, Ave reward -56.64, Ave turns 35.28, Best success rate 0\n",
      "Progress: 7 / 100, Success rate: 0 / 7 Avg reward: -47.00 Avg turns: 16.00\n",
      "----------------- Episode: 7 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 8423\n",
      "- cur bellman err 3.7550, experience replay pool 8423\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 8 / 100, Success rate: 0 / 8 Avg reward: -48.62 Avg turns: 19.25\n",
      "----------------- Episode: 8 ----------------- \n",
      "simulation success rate 0.01, ave reward -58.79, ave turns 41.98\n",
      "Train on : 10522\n",
      "- cur bellman err 2.1125, experience replay pool 10522\n",
      "Simulation success rate 0.01, Ave reward -58.79, Ave turns 41.98, Best success rate 0\n",
      "Progress: 9 / 100, Success rate: 0 / 9 Avg reward: -49.89 Avg turns: 21.78\n",
      "----------------- Episode: 9 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 12622\n",
      "- cur bellman err 1.6403, experience replay pool 12622\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 10 / 100, Success rate: 0 / 10 Avg reward: -50.90 Avg turns: 23.80\n",
      "----------------- Episode: 10 ----------------- \n",
      "Successful Dialog!\n",
      "simulation success rate 0.74, ave reward 36.84, ave turns 25.92\n",
      "simulation success rate 0.82, ave reward 47.29, ave turns 24.22\n",
      "Train on : 1211\n",
      "- cur bellman err 4.8285, experience replay pool 1211\n",
      "Simulation success rate 0.74, Ave reward 36.84, Ave turns 25.92, Best success rate 0\n",
      "Progress: 11 / 100, Success rate: 1 / 11 Avg reward: -40.09 Avg turns: 24.00\n",
      "----------------- Episode: 11 ----------------- \n",
      "simulation success rate 0.0, ave reward -57.6, ave turns 37.2\n",
      "Train on : 3071\n",
      "- cur bellman err 4.8930, experience replay pool 3071\n",
      "Simulation success rate 0.0, Ave reward -57.6, Ave turns 37.2, Best success rate 0\n",
      "Progress: 12 / 100, Success rate: 1 / 12 Avg reward: -40.50 Avg turns: 23.00\n",
      "----------------- Episode: 12 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 5171\n",
      "- cur bellman err 1.7782, experience replay pool 5171\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 13 / 100, Success rate: 1 / 13 Avg reward: -42.00 Avg turns: 24.46\n",
      "----------------- Episode: 13 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 7271\n",
      "- cur bellman err 0.9737, experience replay pool 7271\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 14 / 100, Success rate: 1 / 14 Avg reward: -43.29 Avg turns: 25.71\n",
      "----------------- Episode: 14 ----------------- \n",
      "Successful Dialog!\n",
      "simulation success rate 0.39, ave reward -8.62, ave turns 32.84\n",
      "simulation success rate 0.45, ave reward -0.68, ave turns 31.36\n",
      "Train on : 1568\n",
      "- cur bellman err 2.3316, experience replay pool 1568\n",
      "Simulation success rate 0.39, Ave reward -8.62, Ave turns 32.84, Best success rate 0\n",
      "Progress: 15 / 100, Success rate: 2 / 15 Avg reward: -35.60 Avg turns: 25.20\n",
      "----------------- Episode: 15 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 3668\n",
      "- cur bellman err 1.1022, experience replay pool 3668\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 16 / 100, Success rate: 2 / 16 Avg reward: -37.12 Avg turns: 26.25\n",
      "----------------- Episode: 16 ----------------- \n",
      "Successful Dialog!\n",
      "simulation success rate 0.61, ave reward 21.54, ave turns 25.32\n",
      "simulation success rate 0.57, ave reward 16.15, ave turns 26.5\n",
      "Train on : 1325\n",
      "- cur bellman err 2.9177, experience replay pool 1325\n",
      "Simulation success rate 0.61, Ave reward 21.54, Ave turns 25.32, Best success rate 0\n",
      "Progress: 17 / 100, Success rate: 3 / 17 Avg reward: -30.65 Avg turns: 25.65\n",
      "----------------- Episode: 17 ----------------- \n",
      "simulation success rate 0.37, ave reward -10.05, ave turns 30.9\n",
      "simulation success rate 0.35, ave reward -12.75, ave turns 31.5\n",
      "Train on : 1575\n",
      "- cur bellman err 2.0018, experience replay pool 1575\n",
      "Simulation success rate 0.37, Ave reward -10.05, Ave turns 30.9, Best success rate 0\n",
      "Progress: 18 / 100, Success rate: 3 / 18 Avg reward: -32.28 Avg turns: 26.56\n",
      "----------------- Episode: 18 ----------------- \n",
      "Successful Dialog!\n",
      "simulation success rate 0.78, ave reward 34.49, ave turns 40.22\n",
      "simulation success rate 0.84, ave reward 41.75, ave turns 40.1\n",
      "Train on : 2005\n",
      "- cur bellman err 1.6525, experience replay pool 2005\n",
      "Simulation success rate 0.78, Ave reward 34.49, Ave turns 40.22, Best success rate 0\n",
      "Progress: 19 / 100, Success rate: 4 / 19 Avg reward: -27.37 Avg turns: 27.26\n",
      "----------------- Episode: 19 ----------------- \n",
      "Successful Dialog!\n",
      "simulation success rate 1.0, ave reward 74.2, ave turns 13.6\n",
      "simulation success rate 1.0, ave reward 74.17, ave turns 13.66\n",
      "Train on : 683\n",
      "- cur bellman err 8.4976, experience replay pool 683\n",
      "Simulation success rate 1.0, Ave reward 74.2, Ave turns 13.6, Best success rate 0\n",
      "Progress: 20 / 100, Success rate: 5 / 20 Avg reward: -22.30 Avg turns: 26.60\n",
      "----------------- Episode: 20 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 2783\n",
      "- cur bellman err 3.8910, experience replay pool 2783\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 21 / 100, Success rate: 5 / 21 Avg reward: -24.10 Avg turns: 27.33\n",
      "----------------- Episode: 21 ----------------- \n",
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 4883\n",
      "- cur bellman err 1.7808, experience replay pool 4883\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 22 / 100, Success rate: 5 / 22 Avg reward: -25.73 Avg turns: 28.00\n",
      "----------------- Episode: 22 ----------------- \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "simulation success rate 0.0, ave reward -60.0, ave turns 42.0\n",
      "Train on : 6983\n",
      "- cur bellman err 0.9757, experience replay pool 6983\n",
      "Simulation success rate 0.0, Ave reward -60.0, Ave turns 42.0, Best success rate 0\n",
      "Progress: 23 / 100, Success rate: 5 / 23 Avg reward: -27.22 Avg turns: 28.61\n",
      "----------------- Episode: 23 ----------------- \n"
     ]
    }
   ],
   "source": [
    "performance_records = {}\n",
    "performance_records['success_rate'] = {}\n",
    "performance_records['ave_turns'] = {}\n",
    "performance_records['ave_reward'] = {}\n",
    "\n",
    "best_model = {}\n",
    "best_res = {'success_rate': 0, 'ave_reward':float('-inf'), 'ave_turns': float('inf'), 'epoch':0}\n",
    "\n",
    "curve = []\n",
    "losses = []\n",
    "agent.warm_start = 1\n",
    "run_episodes(100, status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_learning_curve(curve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_curve(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_loss_curve(losses[-100:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saver.save(agent.model.sess, \"trained_model/flight_100/model.ckpt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Initialization Cell",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
